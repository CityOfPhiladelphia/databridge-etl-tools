FROM apache/airflow:3.1.0

# See https://airflow.apache.org/docs/docker-stack/build.html
USER root

# Add our worker users custom binaries to the path, some python packages are installed here.
# ENV PATH="/home/worker/.local/bin:${PATH}"

# Never prompts the user for choices on installation/configuration of packages
#ENV DEBIAN_FRONTEND noninteractive
#ENV TERM linux

RUN apt-get update && \
  apt-get install -y --no-install-recommends \
    libpq-dev \
    libpq5 \
    apt-utils \
    build-essential \
    ca-certificates \
    curl \
    git \
    gnupg \
    lsb-release \
    wget \
    unzip \
  && apt-get autoremove -yqq --purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

USER airflow

# Put our pip installs, but not our own module install, here first so we can more rapidly build.
# pip installs take the longest.
#RUN pip install pip --upgrade \
#  && pip install setuptools --upgrade

# Funny command to install dependencies directly from pyproject.toml, that way we don't have to
# maintain a separate requirements.txt file.
#COPY --chown=worker:root pyproject.toml ./pyproject.toml
#RUN python - <<'PY' >/tmp/pyproject-toml-reqs.txt
#import tomllib as tl
#with open("pyproject.toml","rb") as f:
#    deps = tl.load(f)["project"]["dependencies"]
#print("\n".join(deps))
#PY
#RUN pip install --no-cache-dir -r /tmp/pyproject-toml-reqs.txt
#
#COPY --chown=worker:root scripts/entrypoint.sh ./entrypoint.sh
#COPY --chown=worker:root tests/ ./tests/
#COPY --chown=worker:root databridge_etl_tools ./databridge_etl_tools
COPY pyproject.toml ./pyproject.toml
COPY databridge_etl_tools ./databridge_etl_tools

# Python syntax check
RUN python -m compileall ./databridge_etl_tools

# Install databridge-etl-tools using pyproject.toml
RUN pip install --no-cache-dir "apache-airflow==${AIRFLOW_VERSION}" .

