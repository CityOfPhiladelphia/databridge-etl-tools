FROM apache/airflow:3.1.7

# See https://airflow.apache.org/docs/docker-stack/build.html
USER root

# Add our worker users custom binaries to the path, some python packages are installed here.
# ENV PATH="/home/worker/.local/bin:${PATH}"

# Never prompts the user for choices on installation/configuration of packages
ENV DEBIAN_FRONTEND noninteractive
#ENV TERM linux

RUN apt-get update && \
  apt-get install -y --no-install-recommends \
    libpq-dev \
    libpq5 \
    apt-utils \
    build-essential \
    ca-certificates \
    curl \
    git \
    gnupg \
    lsb-release \
    wget \
    unzip \
  && apt-get autoremove -yqq --purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

USER airflow
WORKDIR /home/airflow/

COPY --chown=airflow:airflow pyproject.toml ./pyproject.toml
COPY --chown=airflow:airflow databridge_etl_tools ./databridge_etl_tools
COPY --chown=airflow:airflow tests/ ./tests/

# Python syntax check
RUN python -m compileall ./databridge_etl_tools

# Install databridge-etl-tools using pyproject.toml
RUN pip install --no-cache-dir "apache-airflow==${AIRFLOW_VERSION}" .

# Set aws access keys as an env var for use with boto3
# do this under the worker user.
# These are passed in via --build-arg at build time.
ARG AWS_ACCESS_KEY_ID
ARG AWS_SECRET_ACCESS_KEY

ENV AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID

ENV AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
